# Комплексный ETL пайплайн с использованием всех созданных плагинов
name: "advanced_data_pipeline"
description: "Продвинутый ETL пайплайн для обработки данных из различных источников"
version: "2.0.0"
author: "Data Engineering Team"
tags: ["etl", "advanced", "multi-source", "analytics"]

# Расписание выполнения
schedule:
  enabled: true
  cron: "0 2 * * *"  # Каждый день в 2:00 утра
  timezone: "UTC"
  overlap_policy: "SKIP"
  catchup_window: "24h"
  pause_on_failure: true
  jitter: "5m"
  notes: "Ночная обработка данных для аналитики"

# Глобальные настройки отказоустойчивости
default_resilience:
  max_attempts: 3
  initial_delay: 2.0
  max_delay: 120.0
  backoff_multiplier: 2.0
  retry_policy: "exponential_backoff"
  jitter: true
  
  # Circuit breaker
  circuit_breaker_enabled: true
  failure_threshold: 5
  recovery_timeout: 300
  success_threshold: 3
  
  # Таймауты
  execution_timeout: 1800  # 30 минут на стадию

# Стадии пайплайна
stages:
  
  # === EXTRACT STAGES ===
  
  # Извлечение пользователей из API
  extract_users_api:
    stage: extract
    component: http_extract
    component_config:
      url: "https://api.example.com/users"
      method: "GET"
      
      # Аутентификация
      auth_config:
        auth_type: "bearer"
        bearer_token: "${API_TOKEN}"
      
      # Пагинация
      pagination_config:
        pagination_type: "page"
        page_param: "page"
        page_size_param: "limit"
        default_limit: 1000
        max_pages: 100
        data_path: "data"
      
      # Дополнительные параметры
      headers:
        "Accept": "application/json"
        "User-Agent": "ETL-Pipeline/2.0"
      
      response_format: "json"
      json_data_path: "data"
      timeout: 60
      retries: 3
      rate_limit: 10.0  # 10 запросов в секунду
    
    # Кастомная конфигурация resilience для API
    resilience:
      max_attempts: 5
      initial_delay: 1.0
      retry_policy: "fibonacci_backoff"
      execution_timeout: 300

  # Извлечение заказов из базы данных
  extract_orders_db:
    stage: extract
    component: sql_extract
    component_config:
      query: |
        SELECT 
          order_id,
          user_id,
          product_id,
          quantity,
          price,
          order_date,
          status,
          shipping_address,
          payment_method,
          created_at,
          updated_at
        FROM orders 
        WHERE updated_at > CURRENT_DATE - INTERVAL '7 days'
          AND status IN ('completed', 'shipped', 'delivered')
        ORDER BY order_date DESC
      
      source_config:
        uri: "${ORDERS_DATABASE_URL}"
        connection_pool_size: 8
        connection_timeout: 45
        query_timeout: 600
      
      batch_size: 5000
      streaming: true

  # Извлечение продуктов из CSV файла
  extract_products_csv:
    stage: extract
    component: csv_extract
    component_config:
      source_config:
        path: "${PRODUCTS_CSV_PATH}"
        encoding: "utf-8"
        timeout: 120
      
      # Параметры CSV
      delimiter: ","
      has_header: true
      skip_rows: 1  # Пропускаем строку с описанием
      
      # Обработка данных
      columns: ["product_id", "name", "category", "price", "description", "stock_quantity"]
      column_mapping:
        "product_id": "id"
        "stock_quantity": "stock"
      
      data_types:
        "id": "int"
        "price": "float"
        "stock": "int"
      
      # Фильтрация
      filter_condition: "pl.col('price') > 0"
      null_values: ["", "NULL", "N/A", "-"]
      batch_size: 4096
      streaming: false

  # Извлечение метаданных из внешнего API
  extract_metadata_api:
    stage: extract
    component: http_extract
    component_config:
      url: "https://metadata-api.example.com/v1/enrichment"
      method: "POST"
      
      auth_config:
        auth_type: "api_key"
        api_key: "${METADATA_API_KEY}"
        api_key_header: "X-API-Key"
      
      headers:
        "Content-Type": "application/json"
      
      response_format: "json"
      timeout: 30
      cache_responses: true
      cache_ttl: 3600

  # === TRANSFORM STAGES ===
  
  # Трансформация и обогащение пользователей
  transform_users:
    stage: transform
    component: json_transform
    depends_on: [extract_users_api]
    component_config:
      
      # Нормализация JSON
      json_normalization:
        json_columns: ["profile", "preferences"]
        max_level: 3
        separator: "_"
        preserve_original: false
      
      # Операции над колонками
      column_operations:
        "full_name": "pl.concat_str([pl.col('first_name'), pl.col('last_name')], separator=' ')"
        "age_group": "pl.when(pl.col('age') < 25).then('young').when(pl.col('age') < 50).then('middle').otherwise('senior')"
        "registration_year": "pl.col('created_at').dt.year()"
      
      # Фильтрация
      filter_conditions:
        - "pl.col('status') == 'active'"
        - "pl.col('email').is_not_null()"
      
      # Обработка пропущенных значений
      fill_null_strategy: "forward"
      drop_null_columns: ["user_id", "email"]
      
      # Дедупликация
      deduplicate: true
      deduplicate_columns: ["email"]
      
      # Метаданные
      add_metadata: true

  # Трансформация заказов с джойном к пользователям
  transform_orders:
    stage: transform
    component: json_transform
    depends_on: [extract_orders_db, transform_users]
    component_config:
      
      # Соединение с пользователями
      join_config:
        join_type: "inner"
        left_on: "user_id"
        right_on: "user_id"
        suffix: "_user"
      
      # Новые колонки
      column_operations:
        "order_value": "pl.col('quantity') * pl.col('price')"
        "order_month": "pl.col('order_date').dt.month()"
        "days_since_order": "(pl.col('current_date') - pl.col('order_date')).dt.days()"
      
      # Агрегация по пользователям
      aggregation:
        group_by: ["user_id", "order_month"]
        aggregations:
          "order_value": "sum"
          "quantity": "sum"
          "order_id": "count"
      
      # Сортировка
      sort_columns: ["user_id", "order_month"]
      sort_descending: false
      
      add_metadata: true

  # Обогащение продуктов
  transform_products:
    stage: transform
    component: json_transform
    depends_on: [extract_products_csv, extract_metadata_api]
    component_config:
      
      # Соединение с метаданными
      join_config:
        join_type: "left"
        left_on: "id"
        right_on: "product_id"
        suffix: "_meta"
      
      # Операции над колонками
      column_operations:
        "price_category": "pl.when(pl.col('price') < 50).then('budget').when(pl.col('price') < 200).then('mid').otherwise('premium')"
        "stock_status": "pl.when(pl.col('stock') > 100).then('in_stock').when(pl.col('stock') > 0).then('low_stock').otherwise('out_of_stock')"
        "profit_margin": "pl.col('price') * 0.3"  # Примерная маржа 30%
      
      # Фильтрация
      filter_conditions:
        - "pl.col('price') > 0"
        - "pl.col('name').is_not_null()"
      
      # Семплирование для тестирования
      sample_fraction: 1.0  # Берем все данные
      
      add_metadata: true

  # Финальная агрегация для аналитики
  transform_analytics:
    stage: transform
    component: json_transform
    depends_on: [transform_orders, transform_products]
    component_config:
      
      # Объединяем заказы с продуктами
      join_config:
        join_type: "inner"
        left_on: "product_id"
        right_on: "id"
        suffix: "_prod"
      
      # Агрегация для дашборда
      aggregation:
        group_by: ["category", "age_group", "order_month"]
        aggregations:
          "order_value": "sum"
          "quantity": "sum"
          "order_id": "count"
          "profit_margin": "sum"
      
      # Сортировка для оптимизации запросов
      sort_columns: ["category", "order_month"]
      
      add_metadata: true

  # === LOAD STAGES ===
  
  # Загрузка пользователей в DWH
  load_users_dwh:
    stage: load
    component: postgres_load
    depends_on: [transform_users]
    component_config:
      connection_config:
        uri: "${DWH_DATABASE_URL}"
        pool_size: 10
        connection_timeout: 60
        ssl_mode: "require"
      
      target_table: "dim_users"
      target_schema: "analytics"
      if_exists: "upsert"
      
      # Upsert конфигурация
      upsert_config:
        conflict_columns: ["user_id"]
        update_columns: ["full_name", "age_group", "status", "updated_at"]
        where_condition: "excluded.updated_at > dim_users.updated_at"
      
      # Параметры загрузки
      batch_size: 2000
      parallel_batches: 3
      
      # Переименование колонок для DWH
      column_mapping:
        "user_id": "id"
        "created_at": "created_date"
        "updated_at": "modified_date"
      
      # Создание индексов
      create_indexes:
        - columns: ["id"]
          unique: true
          name: "idx_dim_users_id"
        - columns: ["email"]
          unique: true
          name: "idx_dim_users_email"
        - columns: ["age_group", "status"]
          name: "idx_dim_users_analytics"
      
      add_load_metadata: true
      validate_data: true

  # Загрузка продуктов в DWH
  load_products_dwh:
    stage: load
    component: postgres_load
    depends_on: [transform_products]
    component_config:
      connection_config:
        uri: "${DWH_DATABASE_URL}"
        pool_size: 8
        connection_timeout: 60
      
      target_table: "dim_products"
      target_schema: "analytics"
      if_exists: "replace"  # Полная перезагрузка продуктов
      
      batch_size: 1500
      
      # Создание таблицы с партицированием
      table_options:
        with_options: "PARTITION BY RANGE (price)"
      
      create_indexes:
        - columns: ["id"]
          unique: true
        - columns: ["category"]
        - columns: ["price_category", "stock_status"]
      
      add_load_metadata: true

  # Загрузка аналитических данных
  load_analytics_dwh:
    stage: load
    component: postgres_load
    depends_on: [transform_analytics]
    component_config:
      connection_config:
        uri: "${DWH_DATABASE_URL}"
        pool_size: 6
        connection_timeout: 60
      
      target_table: "fact_order_analytics"
      target_schema: "analytics"
      if_exists: "append"
      
      batch_size: 3000
      parallel_batches: 2
      
      # Индексы для быстрых запросов
      create_indexes:
        - columns: ["category", "order_month"]
        - columns: ["age_group", "order_month"]
        - columns: ["load_id"]
      
      add_load_metadata: true
      max_errors: 10  # Допускаем некоторые ошибки

  # Загрузка в кеш для API
  load_cache_redis:
    stage: load
    component: redis_load  # Предполагаемый будущий плагин
    depends_on: [load_analytics_dwh]
    component_config:
      connection_config:
        host: "${REDIS_HOST}"
        port: 6379
        password: "${REDIS_PASSWORD}"
        db: 0
      
      key_pattern: "analytics:{category}:{month}"
      ttl: 3600  # 1 час
      format: "json"

# Переменные окружения
required_env_vars:
  - API_TOKEN
  - ORDERS_DATABASE_URL
  - PRODUCTS_CSV_PATH
  - METADATA_API_KEY
  - DWH_DATABASE_URL
  - REDIS_HOST
  - REDIS_PASSWORD

# Глобальные настройки
max_parallel_stages: 4
default_timeout: 1800  # 30 минут

# Уведомления
notifications:
  on_success:
    email: ["data-team@company.com"]
    slack: 
      webhook_url: "${SLACK_WEBHOOK_URL}"
      channel: "#data-pipeline"
  
  on_failure:
    email: ["data-team@company.com", "ops-team@company.com"]
    slack:
      webhook_url: "${SLACK_WEBHOOK_URL}" 
      channel: "#alerts"
    pagerduty:
      integration_key: "${PAGERDUTY_KEY}"

# Метаданные пайплайна
metadata:
  data_sources:
    - "Users API"
    - "Orders Database"
    - "Products CSV"
    - "Metadata API"
  
  destinations:
    - "Analytics DWH"
    - "Redis Cache"
  
  data_volume_estimate: "1M+ records daily"
  execution_time_estimate: "15-30 minutes"
  
  business_impact: "Critical for daily analytics and reporting"
  data_retention: "2 years in DWH, 1 hour in cache"
  
  contact:
    team: "Data Engineering"
    email: "data-team@company.com"
    slack: "#data-engineering"

# Мониторинг и алерты
monitoring:
  metrics:
    - name: "records_processed"
      threshold: 100000
      operator: ">"
    
    - name: "execution_time"
      threshold: 2700  # 45 минут
      operator: "<"
    
    - name: "error_rate"
      threshold: 0.05  # 5%
      operator: "<"
  
  health_checks:
    - name: "api_availability"
      url: "https://api.example.com/health"
      interval: 300  # 5 минут
    
    - name: "database_connection"
      type: "database"
      connection_string: "${ORDERS_DATABASE_URL}"
      interval: 600  # 10 минут

# Конфигурация для разработки
development:
  sample_data: true
  sample_size: 1000
  skip_notifications: true
  log_level: "DEBUG"
  
  # Переопределения для dev среды
  override_configs:
    extract_users_api:
      component_config:
        pagination_config:
          max_pages: 5
    
    load_users_dwh:
      component_config:
        target_schema: "dev_analytics"
